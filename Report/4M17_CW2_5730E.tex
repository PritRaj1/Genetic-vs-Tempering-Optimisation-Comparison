\documentclass[10pt]{article}
\usepackage{titling} % Customize the title
\usepackage[utf8]{inputenc}
\usepackage{eso-pic}
\usepackage{charter}
\usepackage[margin=1in]{geometry}
\usepackage{amssymb,pdfpages,fancyhdr,subcaption,graphicx,hyperref,float,outlines,amsmath,gensymb}
\usepackage{listings}
\usepackage{parskip}
\usepackage{multicol} % Added the multicol package
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{colortbl}

% Define colors for code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Define settings for Python code
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{\textbf{4M17 Coursework \#2 \\ Optimisation Algorithm Performance Comparison}}
\author{\textbf{Candidate No: 5730E}}

\begin{document}
\maketitle
\section{Abstract}
This report conducts a comparative analysis of two optimisation algorithms applied to minimise Keane's Bump Function, (KBF). In particular, the study focuses on a Continuous Genetic Algorithm, (CGA), as well as an alternative algorithm not covered in the lectures: Q-learning, (QEG). Specifically, the Q-learning approach is adapted with the epsilon-greedy strategy to introduce a level of stochasticity into the optimisation process, thus aligning it with the requirements of this assignment.

\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/KBF/KBF_surf.png}
    \caption{Surface plot.}
    \label{fig:KBF_surf}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/KBF/KBF_contour.png}
    \caption{Contour plot.}
    \label{fig:KBF_contour}
\end{subfigure}
    \captionsetup{justification=centering}
    \caption{The two-dimensional visualisation of the Keane's Bump Function, (KBF).}
    \label{fig:KBF_2D}
\end{figure}
\newpage
\section{Introduction}
\begin{multicols}{2}
\subsection{Keane's Bump Function}
To compare the performances of the two algorithms, the Keane's Bump Function, (KBF), is used as the objective function. In particular, the n-dimensional constrained optimisation problem is defined as the maximisation of:

\begin{equation}
    f(\mathbf{x}) = \left| \frac{\sum_{i=1}^{n} (\cos(x_i))^4 - 2\prod_{i=1}^{n} (\cos(x_i))^2}{\sqrt{\sum_{i=1}^{n} i \cdot x_i^2}} \right|
    \label{eq:KBF_cost}
\end{equation}
\begin{equation}
    \begin{aligned}
        \text{subject to} \quad & 0 \leq x_i \leq 10 \quad \forall i \in \{1, \dots, n\} \\
        & \quad \prod_{i=1}^{n} x_i > 0.75 \\
        & \quad \sum_{i=1}^{n} x_i < \frac{15n}{2}
    \end{aligned} 
    \label{eq:KBF_constraints}
\end{equation}

The two-dimensional form of the function has been plotted in Figure \ref{fig:KBF_2D}. Some notable properties are as follows:

\begin{itemize}
    \item The function is undefined at the origin, (0, 0). This is due to the division by zero in the denominator of Equation \ref{eq:KBF_cost}. Otherwise, the function is continuous and differentiable everywhere.
    \item The function is highly multi-modal. Its global maximum is located on the constaint boundary $x_{n}=0$, where $x_n$ denotes the final variable in the n-dimensional space. However, there are many local maxima located inside the feasible region, all of which have quite similar amplitudes.
    \item The function is nearly symmetric about the line $x_1=x_2$. This stems from its construction in \ref{eq:KBF_cost}, using the sums of squared, symmetric terms, $x_i^2$, $(\cos(x_i))^2$, and $(\cos(x_i))^4$. This results in some invariance regarding the order of the input variables. Overall, the peaks consistently manifest in pairs, yet there is a notable pattern wherein one peak always surpasses its counterpart in magnitude.
\end{itemize}

Given the above properties, the KBF is a challenging function to optimise. The presence of multiple, similar-amplitude local maxima makes it difficult for an optimisation algorithm to converge to the global maximum. On the other hand, all control variables share the same nature, (continuous variables), and exhibit identical scales. Additionally, all constraints are of the inequality type, and the feasible space is non-disjoint.

The problem becomes more complicated with the inclusion of the constraints outlined in \ref{eq:KBF_constraints}. Figure \ref{fig:KBF_Feasible} illustrates the resulting feasible region carved out of the original function space. Notably, the contstraint boundaries are non-linear, and the feasible region is non-disjoint. The problem complexity is additionally excaberated by the presence of multiple optima along the constraint boundaries, including the global maxima that we seek to identify.

These properties make the KBF a suitable candidate for the comparative analysis of the two optimisation algorithms, as discussed in the previous work \cite{ELBELTAGY1999639}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{../figures/KBF/KBF Feasible_contour.png}
    \captionsetup{justification=centering}
    \caption{The three-dimensional visualisation of the Keane's Bump Function, (KBF).}
    \label{fig:KBF_Feasible}
\end{figure}

\subsection{Continuous Genetic Algorithm}

The discrete nature of the GA presented in \cite{parks2023geneticalgorithms} makes it unsuitable for the optimisation of the KBF. An implementation of a Continuous Genetic Algorithm, (CGA), is used instead, which lends itself better to the problems presented in \ref{eq:KBF_cost}-\ref{eq:KBF_constraints}.

The CGA, a technique inspired by natural selection and genetics, presents itself as particularly well-suited to tackling challenges associated with multiple local optima. Furthermore, the algorithm lends itself well to parallelisation with low implementation effort, and offers ample opportunities for modifications and adaptations, supported by a rich body of literature on the subject.

The primary difference between the CGA and the GA in \cite{parks2023geneticalgorithms} is the representation of individuals, (solutions of the state space), within the population. Rather than representing an individual as a vector of binary values or bits, (0s and 1s), the CGA uses a real-valued vector of floating-point numbers to represent each individual, as discussed in \cite{PGA}. This allows for a direct representation of the problem, and eliminates the need for a decoding function, which reduces overhead in function evaluations.

This adjustment marks a significant departure from conventional GAs, aligning the algorithm more closely with Evolution Strategies (ES), another member of the evolutionary algorithms family presented in \cite{salimans2017evolution}. However, the algorithm presented in \ref{sec:CGA_implementation} is still classified as a GA in accordance with the differences presented in \cite{10.1007/BFb0029787}, given that mutation does not serve as the primary search mechanism for exploring the state space. Instead, it functions as a non-adaptive, background operator.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{../figures/Ungenerated Images/Flowchart.png}
    \captionsetup{justification=centering}
    \caption{A flowchart depicting the CGA process, taken from \cite{parks2023geneticalgorithms}.}
    \label{fig:GAprocess}
\end{figure}

\subsubsection{Implementation}
\label{sec:CGA_implementation}

In accordance with \cite{parks2023geneticalgorithms}, a vector solution of the state space will be referred to as an \textit{individual} or \textit{chromosome}, and a matrix of individuals will be referred to as a \textit{population}. Each individual is represented as a $n \times 1$ vector of real-valued (floating point) numbers, where $n$ denotes the number of variables in the state space. The population is represented as a $m \times n$ matrix, where $m$ denotes the number of individuals in the population, and is dictated as a hyperparameter within the code: %ref code

In accordance with the terminology presented in \cite{parks2023geneticalgorithms}, a vector solution of the state space will be referred to as an \textit{individual} or \textit{chromosome}. Correspondingly, a collection of such individuals arranged in a matrix format will be denoted as a \textit{population}. Each individual is delineated as an $n \times 1$ vector of real-valued (floating-point) numbers, where $n$ signifies the number of variables in the state space. The population itself is represented as a $m \times n$ matrix, where $m$ designates the count of individuals in the population. This count is explicitly defined as a hyperparameter within the codebase: %placeholder

The CGA process is outlined in Figure \ref{fig:GAprocess}. 

\textbf{\underline{Initialisation}} 

The population is initialised with random values uniformly distributed within the bounds of the state space.

\textbf{\underline{Parent Selection}} 

Three selection methods were implemented: proportional selection, tournament selection, and stochastic remainder selection without replacement (SRS). These are outlined and compared in Section \ref{sec:CGA_selection_mutation}. 

Three strategies were deliberately implemented to harness the flexibility inherent in the CGA, tailoring it to the optimisation challenges posed by the KBF. The subsequent section, \ref{sec:CGA_selection_mutation}, elucidates the selection method and mutation procedure choices that form the basis of the forthcoming comparison.

\textbf{\underline{Mating Procedure}} 

Similarly, two mating procedures were implemented: crossover and blending. These are also outlined and compared in Section \ref{sec:CGA_selection_mutation}.

\textbf{\underline{Mutation}} 

The mutation procedure proposed by \cite{PGA} involves perturbing a gene within a chromosome by a normally distributed random number with a mean of zero and a standard deviation of $\sigma$. However, the effectiveness of this procedure is constrained by the selection of $\sigma$ as an additional hyperparameter, necessitating careful tuning. 

Instead, a simpler approach was adopted, where genes within the population have an probability, given by the mutation rate, of being reset to a random value drawn from a uniform distribution within the confines of the state space, mirroring the initialisation procedure.

\textbf{\underline{Evaluate}} 

The population is then evaluated, and the individuals are ranked in order of fitness. Here, the fitness is defined as the negative of the KBF cost function, outlined in \ref{eq:KBF_cost}. This is done to align the algorithm with the maximisation of the KBF.

\textbf{\underline{Termination}} 

For this section of the report, the algorithm terminates after a maximum number of iterations, (defined as a hyperparameter). A more stringent convergence criterion is adopted for the comparison between the two algorithms in Section \ref{sec:CGA_QEG_comparison}.

\subsubsection{Choosing the Selection Method and Mutation Procedure}
\label{sec:CGA_selection_mutation}

Given the flexibility of the CGA, multiple selection methods and mutation procedures were implemented within the codebase. These are then selected as hyperparameters within the script, %placeholder
to take advantage of the flexibility offered by the CGA. In particular, the following selection methods were implemented:

\textbf{\underline{Proportional Selection}}

The probability of an individual being selected for mating is proportional to its fitness:
\[
    p_i = \frac{f_i}{\sum_{i=1}^{n} f_i}
\]
Here, \(f_i\) denotes the fitness of the \(i\)th individual. This is the simplest selection method, and is implemented in accordance with the theory presented in \cite{parks2023geneticalgorithms}.

\textbf{\underline{Tournament Selection}}

As outlined in \cite{parks2023geneticalgorithms}, this strategy involves taking a small subset of the population, and selecting the top two individual with the highest fitness. Selection pressure can then be adjusted by varying the size of the subset, controlled by the \textit{TOURNAMENT$\_$SIZE} hyperparameter.

This is a popular selection method, as it is simple to implement, and is known to perform well in practice. It can be improved by including some of the concepts presented in \cite{Miller1995GeneticAT}, however, these have been avoided considering \cite{parks2023geneticalgorithms} suggest SRS as a superior alternative.

\textbf{Stochastic Remainder Selection} \\
\textbf{\underline{without Replacement (SRS)}}

This strategy draws inspiration from \cite{parks2023geneticalgorithms}. Specifically, a group of chosen individuals is curated by generating an expected number of copies for each individual, denoted as: 
\[E_i = N * p_i\]
Here, \(N\) represents the population size, and \(p_i\) is elucidated above in the context of proportional selection. The anticipated number of duplicates is subsequently divided into an integer part, \(I_i = \lfloor E_i \rfloor\), and a remainder, \(R_i = E_i - I_i\). 

The integer part is used for deterministic selection of individuals, while the remainder is employed for stochastic selection. The two parents are then randomly sampled from this augmented collection.

The discussion in \cite{parks2023geneticalgorithms} highlights that this approach appears to yield superior performance, ascribed to the inclusion of a degree of determinism in the selection criteria. Nevertheless, it remains worthwhile to evaluate the performance of the other two methods, as they might present distinct advantages within the framework of the KBF.

\end{multicols}
\begin{table}[H]
    \centering
    \begin{tabular}{|*{5}{c|}}
        \hline
        \renewcommand{\arraystretch}{1.5}
        \multirow{2}{*}{\textbf{Selection Method}} & \multirow{2}{*}{\textbf{Mating Procedure}} & \multirow{2}{*}{\textbf{Iterations}} & \multirow{2}{*}{\textbf{Final Avg. Fitness}} & \multirow{2}{*}{\textbf{Final Min. Fitness}} \\
        & & & & \\
        \hline
        \multirow{4}{*}{Proportional} & \multirow{2}{*}{Crossover} & 10 & -0.2218 & -0.2875 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.2385 &\cellcolor{lightgray} -0.2835 \\
        \cline{2-5}
        & \multirow{2}{*}{Blending} & 10 & -0.2277 & -0.2629 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.2341 & \cellcolor{lightgray} -0.2628 \\
        \hline
        \multirow{4}{*}{Tournament} & \multirow{2}{*}{Crossover} & 10 & -0.0215 & -0.2157 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.0100 &\cellcolor{lightgray} -0.2451 \\
        \cline{2-5}
        & \multirow{2}{*}{Blending} & 10 & -0.0516 & -0.1818 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.0593 &\cellcolor{lightgray} -0.1979 \\
        \hline
        \multirow{4}{*}{SRS} & \multirow{2}{*}{Crossover} & 10 & -0.2197 & -0.4333 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.2437 &\cellcolor{lightgray} -0.2815 \\
        \cline{2-5}
        & \multirow{2}{*}{Blending} & 10 & -0.2295 & -0.2629 \\
        & &\cellcolor{lightgray} 100 &\cellcolor{lightgray} -0.2360 &\cellcolor{lightgray} -0.2625 \\
        \hline
    \end{tabular}
    \caption{Your Table Caption Here}
    \label{tab:mytable}
\end{table}



\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/SRS/Crossover/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Crossover}
        \label{fig:CGA_flowchart_SRS_crossover}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/SRS/Blending/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Blending}
        \label{fig:CGA_flowchart_SRS_blending}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{Evolution of the population over 10 iterations using Stochastic Remainder Selection without Replacement (SRS). SRS was found to be the most effective selection method, when compared to Proportional Selection and Tournament Selection, as seen in Figures \ref{fig:CGA_flowchart_proportional} and \ref{fig:CGA_flowchart_tournament}, respectively.}
    \label{fig:CGA_flowchart_SRS}
\end{figure}


\section{Methodology}
\label{sec:CGA_QEG_comparison}
\section{Results}
\section{Discussion}
\section{Conclusion}
\bibliographystyle{plain}
\bibliography{refs} % Entries are in the refs.bib file
\newpage
\section{Appendix}
\subsection{Supplementary Figures}
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/Proportional/Crossover/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Crossover}
        \label{fig:CGA_flowchart_proportional_crossover}
    \end{subfigure}
    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/Proportional/Blending/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Blending}
        \label{fig:CGA_flowchart_proportional_blending}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{CGA evolution of the population over 10 iterations using proportional selection. Proportional selection proved to be an effective selection method. However, it was not chosen over SRS as the primary selection method for the reasons outlined in Section \ref{sec:CGA_selection_mutation}.}
    \label{fig:CGA_flowchart_proportional}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/Tournament/Crossover/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Crossover}
        \label{fig:CGA_flowchart_tournament_crossover}
    \end{subfigure}
    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/KBF/10_iters/Tournament/Blending/0.05_0.7_Population.png}
        \caption{Mutation Procedure: Blending}
        \label{fig:CGA_flowchart_tournament_blending}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{CGA evolution of the population over 10 iterations using tournament selection. Using tournament selection, the algorithm was unable to converge to a solution. Even 100 iterations produced a suboptimal solution, showcasing convergences to local optima, rather than the global optimum. As such, it was disregarded as a viable selection method.}
    \label{fig:CGA_flowchart_tournament}
\end{figure}

\end{document}